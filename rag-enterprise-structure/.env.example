# ============================================================
# RAG ENTERPRISE CONFIGURATION
# ============================================================

# =====  DATABASE  =====
# Milvius
MILVIUS_HOST=milvius
MILVIUS_PORT=19530

# =====  BACKEND  =====
# LLM Model
LLM_MODEL=mistral
# Opzioni: mistral, llama2, neural-chat, dolphin-mixtral, etc.

# Embedding Model
EMBEDDING_MODEL=BAAI/bge-m3
# Opzioni (ordinati per performance):
#   - BAAI/bge-m3 (Multilingual SOTA, dense+sparse+colbert, 2.3GB) [RECOMMENDED]
#   - BAAI/bge-large-en-v1.5 (English SOTA, 1.3GB)
#   - intfloat/e5-large-v2 (Multilingual high perf, 1.3GB)
#   - sentence-transformers/all-roberta-large-v1 (English high quality, 1.3GB)
#   - deepseek-ai/deepseek-coder-6.7b-base (Code specialized, 13GB)
#   - all-mpnet-base-v2 (English medium quality, 430MB)
#   - multilingual-e5-large (Multilingual medium, 1.3GB)
#   - all-MiniLM-L6-v2 (English basic, 22MB, veloce)
#   - multilingual-MiniLM-L6-v2 (Multilingual basic, 61MB)

# GPU Configuration
CUDA_VISIBLE_DEVICES=0
# Usa GPU 0 (default), o "0,1" per multipli GPU

# =====  FRONTEND  =====
# URL del backend (per frontend remoto)
BACKEND_URL=http://backend:8000
# Esempi:
#   - Local: http://localhost:8000
#   - Remote: http://backend-server.datacenter.com:8000

# API Key
API_KEY=sk-rag-enterprise

# =====  OPTIONAL: ADVANCED  =====
# OCR Language (per PaddleOCR)
OCR_LANG=en
# Opzioni: en, it, zh, ja, ko, etc.

# LLM Temperature (creatività risposte)
LLM_TEMPERATURE=0.7
# Range: 0.0 (deterministic) - 1.0 (creative)

# Embedding Batch Size
EMBEDDING_BATCH_SIZE=32
# Aumenta se hai VRAM disponibile per velocità

# Max Upload Size (MB)
MAX_UPLOAD_SIZE=100
